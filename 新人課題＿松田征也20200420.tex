\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage[dvipdfmx]{graphicx}
\usepackage[dvipdfmx]{color}

\usepackage{listings, jlisting}
\usepackage{color}
\usepackage{here}
\lstset{
    language=Python,
    backgroundcolor={\color[gray]{.90}},
    basicstyle={\ttfamily\footnotesize},
    identifierstyle={\small},
    commentstyle={\small\ttfamily \color[rgb]{0,0.5,0}},
    keywordstyle={\small\bfseries \color[rgb]{0,0,0.8}},
    ndkeywordstyle={\small},
    stringstyle={\small\ttfamily \color[rgb]{0,0,1}},
    frame={single},
    breaklines=true,
    columns=[l]{fullflexible},
    numbers=left,
    xrightmargin=0zw,
    xleftmargin=3zw,
    numberstyle={\scriptsize},
    stepnumber=1,
    numbersep=1zw,
    morecomment=[l]{//}
}

% タイトル
\title{1. 画像処理の基礎}
\author{松田　征也}
\date{2020/04/17}

\setcounter{section}{-1}% 節番号を0から始める

\begin{document}

\maketitle

\section{python+OpenCVのプログラミング環境構築}
    
    課題においてanacondaを使用してPython環境の構築をあらかじめ行った.
    
    Anacondaの仮想環境にOpenCVの環境を構築するため，以下のcondaコマンドを入力した．
    
    \begin{verbatim}
        conda install -c menpo opencv
    \end{verbatim}
    
    統合開発プラットフォームはJupyter Notebookを使用した．
    実験環境は以下に示す．
    
    \begin{itemize}
        \item MacOS Catalina バージョン 10.15.4
        \item Anaconda3 20.02 Python3.7
        \item numpy 1.18.1
        \item opencv 3.4.2
        \item matplotlib 3.1.3
    \end{itemize}

\section{numpyを使った行列の四則演算}

行列$A$，$B$およびスカラー値$k$を定義して，以下の5つの演算を試みる．

\begin{itemize}
    \item 行列の和
    \item 行列の差
    \item 行列の積
    \item スカラー積
    \item アダマール積
\end{itemize}

今回，$A$，$B$，$c$の値を以下の式(\ref{para})のように定める．

\begin{equation}
\label{para}
    A = \left(
    \begin{array}{cc}
      2.0 & 1.0 \\
      1.5 & 1.3
    \end{array}
  \right), 
   B = \left(
    \begin{array}{cc}
      4.0 & 3.0 \\
      8.0 & 1.0 
    \end{array}
  \right),
  c=2
\end{equation}

それぞれの5つの演算の結果は以下の式(\ref{plus})から式(\ref{hadamard})のようになる

\begin{itemize}
    \item 行列の和
        \begin{equation}
        \label{plus}
            A + B = \left(
            \begin{array}{cc}
              6.0& 4.0 \\
              9.5 & 2.3
            \end{array}
            \right)
        \end{equation}
        
    \item 行列の差
        \begin{equation}
        \label{minus}
            A - B = \left(
            \begin{array}{cc}
              -2.0 & -2.0 \\
              -6.5 & 0.3 
            \end{array}
            \right)
        \end{equation}
    
    \item 行列の積    
        \begin{equation}
        \label{times}
            AB = \left(
            \begin{array}{cc}
              8.0 & 3.0 \\
              12.0 & 1.3
            \end{array}
            \right)
        \end{equation}
    
    \item スカラー積    
        \begin{equation}
        \label{scalar}
            cA = \left(
            \begin{array}{cc}
             4.0 & 2.0 \\
             3.0 & 2.6
            \end{array}
            \right)
        \end{equation}
    
    \item アダマール積
        \begin{equation}
        \label{hadamard}
            A \otimes B = 
            \left(
            \begin{array}{cc}
              16.0 & 7.0 \\
              16.4 & 5.8
            \end{array}
            \right)
        \end{equation}

\end{itemize}

\subsection{ソースコード}
\begin{lstlisting}
import numpy as np
A=np.array([[2.0,1.0],[1.5,1.3]])
B=np.array([[4.0,3.0],[8.0,1.0]])
c=2
print("A+B=\n{}".format(A+B))
print("A-B=\n{}".format(A-B))
print("A*B=\n{}".format(A*B))
print("A*c=\n{}".format(A*c))
print("A@B=\n{}".format(A@B))
\end{lstlisting}

\subsection{実行結果}
\begin{verbatim}
A+B=
[[6.  4. ]
 [9.5 2.3]]
A-B=
[[-2.  -2. ]
 [-6.5  0.3]]
A*B=
[[ 8.   3. ]
 [12.   1.3]]
A*c=
[[4.  2. ]
 [3.  2.6]]
A@B=
[[16.   7. ]
 [16.4  5.8]]
\end{verbatim}

\section{画像の表示，縮小拡大，回転，二値化}

OpenCVを用いて画像の表示，拡大，縮小，回転，二値化を行った．画像は図\ref{01}を使用した．

\begin{figure}[H]
\centering
\includegraphics[width=8cm]{img/01.png}
\caption{画像処理に用いる画像}
\label{01}
\end{figure}

\subsection{ソースコード}

\begin{lstlisting}
import cv2
from IPython.display import Image, display
##画像の表示
def imshow(img):
    '''画像を Notebook 上にインラインで表示する．
    '''
    img = cv2.imencode('.png', img)[1]
    display(Image(img))
img = cv2.imread('IMG_7562.jpg')
imshow(img)
img.shape[:2]
##画像の拡大
height,width=img.shape[:2]
img_double=cv2.resize(img,(width*2,height*2))
imshow(img_double)
img_double.shape[:2]
##画像の縮小
height,width=img.shape[:2]
img_half=cv2.resize(img,(int(width/2),int(height/2)))
imshow(img_half)
img_half.shape[:2]
#画像の回転
#回転の中心を定義
center=(int(width/2),int(height/2))
#回転角を指定
angle = 90.0
#スケールを指定
scale = 1.0
#getRotationMatrix2D関数を使用
trans = cv2.getRotationMatrix2D(center,angle,scale)
#アフィン変換
image_ro = cv2.warpAffine(img, trans, (width,height))
imshow(image_ro)
image_ro.shape[:2]
##画像の二値化
img_gray=cv2.imread("IMG_7562.JPG",0)

# 閾値の設定
threshold = 100

# 二値化(閾値100を超えた画素を255にする．)
ret, img_thresh = cv2.threshold(img_gray, threshold, 255, cv2.THRESH_BINARY)

# 二値化画像の表示
imshow(img_thresh)
\end{lstlisting}

\subsection{実行結果}

ソースコード中で作成した関数imshow()により表示させた4つの画像を示す．

\begin{figure}[H]
\centering
\includegraphics[width=8cm]{img/02.png}
\caption{二倍拡大画像}
\label{02}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=8cm]{img/03.png}
\caption{二分の一縮小画像}
\label{03}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=8cm]{img/04.png}
\caption{反時計回りに90度回転した画像}
\label{04}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=8cm]{img/05.png}
\caption{二値化した画像}
\label{05}
\end{figure}

\section{2枚の異なる画像の差分画像作成}

2枚の異なる画像から差分画像を算出した．
ここでは差分画像は，OpenCVの背景差分計算用オブジェクトであるcreateBackgroundSubtractorMOG2関数を使用した．

使用した画像を以下の図\ref{06}と図\ref{07}に示す．

\begin{figure}[H]
\centering
\includegraphics[width=10cm]{img/06.png}
\caption{画像1}
\label{06}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=10cm]{img/07.png}
\caption{画像2}
\label{07}
\end{figure}

\subsection{ソースコード}

\begin{lstlisting}
#「背景」となる画像の取り込み（グレースケール）
img_src01 = cv2.imread("sample01.jpg", 0)
imshow(img_src01)
#「差分」をもった画像の取り込み（グレースケール）
img_src02 = cv2.imread("sample02.jpg", 0)
imshow(img_src02)
#「背景差分」計算用オブジェクトの作成　
bgObj = cv2.createBackgroundSubtractorMOG2()

#差分となっている「前景領域」に対してマスクをかける
fgmask = bgObj.apply(img_src01)
fgmask = bgObj.apply(img_src02)

#画面に表示
imshow(fgmask)

#「差分」画像のファイル名
bg_diff_path = "./diff.jpg"

#「差分」画像の保存
cv2.imwrite(bg_diff_path, fgmask)

\end{lstlisting}

\subsection{出力結果}

出力された差分画像を図\ref{08}に示す．

\begin{figure}[H]
\centering
\includegraphics[width=10cm]{img/08.png}
\caption{1と2の差分画像}
\label{08}
\end{figure}

\section{画像の特徴量抽出と図示}

\subsection{ヒストグラム}

画像のヒストグラムとは，画像内のR値，B値，G値の度数分布を表したものである．

Numpyのhistogram関数を使用してヒストグラムを作成し,matplotlibを用いてグラフに表示した.
ヒストグラムの作成に使用した画像は図\ref{hist}を使用する．


\begin{figure}[H]
    \centering
    \includegraphics[width=10cm]{img/hist.jpg}
    \caption{ヒストグラムの作成に使用した画像}
    \label{hist}
    \end{figure}
\subsubsection{ソースコード}

\begin{lstlisting}
# 入力画像を読み込み
img = cv2.imread("IMG_0297.JPG")
    
b, g, r = img[:,:,0], img[:,:,1], img[:,:,2]

# 方法1(NumPyでヒストグラムの算出)
hist_r, bins = np.histogram(r.ravel(),256,[0,256])
hist_g, bins = np.histogram(g.ravel(),256,[0,256])
hist_b, bins = np.histogram(b.ravel(),256,[0,256])

# 方法2(OpenCVでヒストグラムの算出)
#hist_r = cv2.calcHist([r],[0],None,[256],[0,256])
#hist_g = cv2.calcHist([g],[0],None,[256],[0,256])
#hist_b = cv2.calcHist([b],[0],None,[256],[0,256])

# グラフの作成
plt.xlim(0, 255)
plt.plot(hist_r, "-r", label="Red")
plt.plot(hist_g, "-g", label="Green")
plt.plot(hist_b, "-b", label="Blue")
plt.xlabel("Pixel value", fontsize=20)
plt.ylabel("Number of pixels", fontsize=20)
plt.legend()
plt.grid()
plt.show
\end{lstlisting}

\subsubsection{出力結果}

出力結果を図\ref{09}に示す．

\begin{figure}[H]
\centering
\includegraphics[width=10cm]{img/09.png}
\caption{作成したヒストグラム}
\label{09}
\end{figure}

\subsection{特徴量抽出}

二種類の画像（目的画像，全体画像）に対してOpenCVを用いて特徴量抽出を行う．特徴量抽出にはSURFのアルゴリズムを使用した．
特徴点はキーポイント(画像上の1ピクセル)と各キーポイントに対応する特徴ベクトルから構成されている．
抽出された特徴量を視覚的表すことを目標とするため，OpenCVのdrawKeypoints関数により特徴ベクトルのノルムの大きさに応じて円を表示させる．

\subsubsection{ソースコード}

\begin{lstlisting}
#一つ目の画像
img1= cv2.imread('IMG_8814.JPG')
surf = cv2.xfeatures2d.SURF_create(400)
kp1 = surf.detect(img1)
img1_sift = cv2.drawKeypoints(img1, kp1, None, flags=4)
imshow(img1_sift)
img_surf1= "./surf1.jpg"
cv2.imwrite(img_surf1,img1_sift)
#二つ目の画像
img2= cv2.imread('IMG_8815.JPG')
surf = cv2.xfeatures2d.SURF_create(400)
kp2 = surf.detect(img2)
img2_sift = cv2.drawKeypoints(img2, kp2, None, flags=4)
imshow(img2_sift)
img2_surf= "./surf2.jpg"
cv2.imwrite(img2_surf,img2_sift)
\end{lstlisting}


\subsection{実行結果}
全体画像と目的画像に対し，SURFを用いて特徴量抽出を行った結果をそれぞれ図\ref{11},図\ref{12}に示す．

\begin{figure}[H]
    \centering
    \includegraphics[width=5cm]{img/11.png}
    \caption{目的画像}
    \label{11}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=10cm]{img/12.png}
    \caption{全体画像}
    \label{12}
\end{figure}


\subsection{特徴点のマッチング}
次に目的画像であるコップを，全体画像から検出する手法を検討する.

\begin{enumerate}
    \item 目的画像および全体画像をグレースケール画像に変換する
    \item それぞれの画像に対し，SIFT特徴量,SURF特徴量,AKAZE特徴量を抽出する．
    \item bf.knnMatchにそれぞれの画像の特徴量記述子(128次元ベクトル)を渡し距離を計算する．
    \item $k=2$，$ratio=0.6$と設定し，des1のそれぞれの点に対して、最も近いdes2の点2つを選び、マッチングインスタンスを出力する．
    \item 最も近い点が2番目に近い点の距離の0.6倍以下の場合採択する．
\end{enumerate}

以上の手順から，最後まで残った目的画像と全体画像の特徴点の組をマッチング結果とする．

\subsubsection{ソースコード}

\begin{lstlisting}
img1 = cv2.imread('IMG_8814.jpg',0)
img2 = cv2.imread('IMG_8815.jpg',0)
#特徴抽出機の生成(SIFT,SURF,AKAZE)
detector = cv2.xfeatures2d.SIFT_create()
#detector = cv2.xfeatures2d.SURF_create()
#detector = cv2.AKAZE_create()
#kpは特徴的な点の位置 destは特徴を現すベクトル
kp1, des1 = detector.detectAndCompute(img1, None)
kp2, des2 = detector.detectAndCompute(img2, None)
#特徴点の比較機
bf = cv2.BFMatcher()
matches = bf.knnMatch(des1,des2, k=2)
#割合試験を適用
good = []
match_param = 0.6
for m,n in matches:
    if m.distance < match_paramam*n.distance:
        good.append([m])
#cv2.drawMatchesKnnは適合している点を結ぶ画像を生成する
img3 = cv2.drawMatchesKnn(img1,kp1,img2,kp2,good,False,flags=10)
imshow(img3)
\end{lstlisting}

\subsubsection{出力結果}

SHIFTによる特徴マッチングの結果を以下の図\ref{SHIFT},SURFによる特徴マッチングの結果を以下の図\ref{SURF},AKAZEによる特徴マッチングの結果を以下の図\ref{AKAZE}に示す．

\begin{figure}[H]
\centering
\includegraphics[width=\hsize]{img/SHIFT.png}
    \caption{SHIFTによる特徴マッチングの結果}
\label{SHIFT}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=\hsize]{img/SURF.png}
    \caption{SURFによる特徴マッチングの結果}
\label{SURF}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=\hsize]{img/SHIFT.png}
    \caption{AKAZEによる特徴マッチングの結果}
\label{AKAZE}
\end{figure}

どの特徴抽出機を使用した場合も正しく視覚的な検出が行えた．
\section{2.医用画像診断}
\subsection{二値分類手法の提案}
本課題では今後に向けて,CNN(畳み込みニューラルネットワーク)
を使用してMRI画像の腫瘍の有無による識別を行うアルゴリズムを作成する．アルゴリズムを作成するにあたり統合開発環境はGoogleが無償で提供しているGoogle colab
を使用した．実験環境は以下に示す．
\begin{itemize}
    \item MacOS Catalina バージョン 10.15.4
    \item Python3.6
    \item numpy 1.18.1
    \item pandas 1.0.3
    \item matplotlib 3.1.3
    \item sklearn 0.22.2.post1
    \item keras 2.3.1 
\end{itemize}
\subsection{データセットの準備}
与えられたデータセットは訓練データ8980枚，
検証データ1448枚，テストデータ2458枚である．これらのデータセットをNumpy配列として読み込み,テキストファイルとして保存した．

\subsubsection{ソースコード}
\begin{lstlisting}
    import numpy as np
    from PIL import Image
    import os
    import sys
    def readImg(paths):
      N = len(paths)
      # 画像読み込み準備
      imgs = [[] for i in range(N)]
      # 正解データ作成
      imgs_targets = []
      for k, path in enumerate(paths):
        #label = 画像が入ってるフォルダ名
        label = os.path.basename(os.path.dirname(path))
        if label == "0":
          imgs_targets.append(0)
        else:
          imgs_targets.append(1)
        imgs[k] = np.asarray(Image.open(path))
        sys.stderr.write('{}枚目\r'.format(k))
        sys.stderr.flush()
      sys.stderr.write('\n')
      imgs = np.array(imgs, dtype = "float32")
      imgs_targets = np.array(imgs_targets, dtype = "int32")
      return imgs, imgs_targets
    from glob import glob
    # データセットのあるパス
    main_path = "/content/drive/My Drive/Colab Notebooks/datasets/"
    train_path = main_path + "train/"
    val_path = main_path + "val/"
    test_path = main_path + "test/"
    # 全画像のパス読み込み
    train_paths = np.array(sorted(glob(train_path + "**/*.png")))
    val_paths = np.array(sorted(glob(val_path + "**/*.png")))
    test_paths = np.array(sorted(glob(test_path + "**/*.png")))
    print(len(train_paths), len(val_paths), len(test_paths))
    x_val, y_val = readImg(val_paths)
    x_test, y_test= readImg(test_paths)
    x_train,y_train=readImg(train_paths)
    np.save(main_path + "x_val", x_val)
    np.save(main_path + "y_val", y_val)
    np.save(main_path + "x_test", x_test)
    np.save(main_path + "y_test", y_test)
    np.save(main_path + "x_train", x_train)
    np.save(main_path + "y_train", y_train)
\end{lstlisting}
\subsection{データの呼び出し及び前処理}
与えられたデータセットの特徴量は０〜255の範囲で与えられているため，0〜1の範囲に収まるように正規化を行う．
また，データセットは３次元テンソルの形状をしているため，４次元テンソルに変換する処理を行った．また，データセットはラベルから順に整列されているためデータセットの順序をランダムに並び替える処理を行った．
\subsubsection{ソースコード}
\begin{lstlisting}
import numpy as np
x_val = np.load(main_path + "x_val.npy")
y_val = np.load(main_path + "y_val.npy")
x_train= np.load(main_path + "x_train.npy")
y_train= np.load(main_path + "y_train.npy")
x_test= np.load(main_path + "x_test.npy")
y_test= np.load(main_path + "y_test.npy")
x_train=x_train/255
x_val=x_val/255
x_test=x_test/255
x_train=x_train.reshape(8980,224,224,1)
x_val=x_val.reshape(1448,224,224,1)
x_test=x_test.reshape(2458,224,224,1)
def shuffle(x,y):
  np.random.seed(1)
  np.random.shuffle(x)
  np.random.seed(1)
  np.random.shuffle(y)
shuffle(x_train,y_train)
shuffle(x_val,y_val)
shuffle(x_test,y_test)
\end{lstlisting}
\subsection{CNNモデルの構築}
構築したCNNモデルはConv2D層とMaxPooling２D層のスタックである．このCNNモデルはサイズ（224,224,1）の入力を処理するように設定している．
また，腫瘍ありか腫瘍なしを判別する２クラス分類であるため、モデルの最終出力は1ユニットであり活性化関数にはシグモイド関数を用いた．過学習の抑制を行うためにコールバック関数としてEarlyStopping関数を使用している．
\subsubsection{ソースコード}
\begin{lstlisting}
import keras
from keras.utils import np_utils
from keras.layers.convolutional import Conv2D, MaxPooling2D
from keras.models import Sequential
from keras.layers.core import Dense, Dropout, Activation, Flatten
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt
model = Sequential()

model.add(Conv2D(32, (3, 3), padding='same',
                 input_shape=(224,224,1)))
model.add(Activation('relu'))
model.add(Conv2D(32, (3, 3)))
model.add(Activation('relu'))
model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(Dropout(0.25))
model.add(Conv2D(64, (3, 3), padding='same'))
model.add(Activation('relu'))
model.add(Conv2D(64, (3, 3)))
model.add(Activation('relu'))
model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(Dropout(0.25))
model.add(Flatten())
model.add(Dense(512))
model.add(Activation('relu'))
model.add(Dropout(0.5))
model.add(Dense(1))      
model.add(Activation('sigmoid'))
model.compile(loss='binary_crossentropy',
             optimizer="rmsprop",
              metrics=['accuracy'])
es_cb = keras.callbacks.EarlyStopping(monitor='val_loss', patience=0, verbose=0, mode='auto')
tb_cb = keras.callbacks.TensorBoard(log_dir="./logs", histogram_freq=1)
history = model.fit(x_train, y_train, batch_size=128, epochs=5,
                   validation_data = (x_val, y_val),verbose=1,callbacks=[es_cb, tb_cb])
model.save(main_path+"kadai2.h5")
\end{lstlisting}
\subsubsection{実行結果}
\begin{lstlisting}
Using TensorFlow backend.
Train on 8980 samples, validate on 1448 samples
Epoch 1/5
8980/8980 [==============================] - 53s 6ms/step - loss: 0.6663 - accuracy: 0.7389 - val_loss: 0.5640 - val_accuracy: 0.7762
Epoch 2/5
8980/8980 [==============================] - 49s 5ms/step - loss: 0.4012 - accuracy: 0.8297 - val_loss: 0.4952 - val_accuracy: 0.7762
Epoch 3/5
8980/8980 [==============================] - 49s 5ms/step - loss: 0.2998 - accuracy: 0.8867 - val_loss: 0.4067 - val_accuracy: 0.8391
Epoch 4/5
8980/8980 [==============================] - 49s 6ms/step - loss: 0.1903 - accuracy: 0.9322 - val_loss: 0.5079 - val_accuracy: 0.8225
\end{lstlisting}
\subsubsection{構築したCNNモデルのアーキテクチャ}
\begin{lstlisting}
Model: "sequential_1"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
conv2d_1 (Conv2D)            (None, 224, 224, 32)      320       
_________________________________________________________________
activation_1 (Activation)    (None, 224, 224, 32)      0         
_________________________________________________________________
conv2d_2 (Conv2D)            (None, 222, 222, 32)      9248      
_________________________________________________________________
activation_2 (Activation)    (None, 222, 222, 32)      0         
_________________________________________________________________
max_pooling2d_1 (MaxPooling2 (None, 111, 111, 32)      0         
_________________________________________________________________
dropout_1 (Dropout)          (None, 111, 111, 32)      0         
_________________________________________________________________
conv2d_3 (Conv2D)            (None, 111, 111, 64)      18496     
_________________________________________________________________
activation_3 (Activation)    (None, 111, 111, 64)      0         
_________________________________________________________________
conv2d_4 (Conv2D)            (None, 109, 109, 64)      36928     
_________________________________________________________________
activation_4 (Activation)    (None, 109, 109, 64)      0         
_________________________________________________________________
max_pooling2d_2 (MaxPooling2 (None, 54, 54, 64)        0         
_________________________________________________________________
dropout_2 (Dropout)          (None, 54, 54, 64)        0         
_________________________________________________________________
flatten_1 (Flatten)          (None, 186624)            0         
_________________________________________________________________
dense_1 (Dense)              (None, 512)               95552000  
_________________________________________________________________
activation_5 (Activation)    (None, 512)               0         
_________________________________________________________________
dropout_3 (Dropout)          (None, 512)               0         
_________________________________________________________________
dense_2 (Dense)              (None, 1)                 513       
_________________________________________________________________
activation_6 (Activation)    (None, 1)                 0         
=================================================================
Total params: 95,617,505
Trainable params: 95,617,505
Non-trainable params: 0
_________________________________________________________________
\end{lstlisting}
\subsection{テストデータによる評価結果}
作成したモデルをテストデータを用いて評価する．
\subsubsection{ソースコード}
\begin{lstlisting}
#作成済みのモデルを呼び出す
import keras
model_new=keras.models.load_model("/content/drive/My Drive/Colab Notebooks/datasets/kadai2.h5")
test_loss,test_acc=model_new.evaluate(x_test,y_test)
test_acc
\end{lstlisting}
\subsubsection{実行結果}
\begin{lstlisting}
2458/2458 [==============================] - 5s 2ms/step
0.8852725625038147
\end{lstlisting}
テストデータでの認識率は約8９％となった．VGG16を使用した場合の認識率（85％〜90％）と比べて妥当な認識率と考えられる．
\subsection{混同行列の出力及び画像の傾向解析}
\subsubsection{ソースコード}
\begin{lstlisting}
import numpy as np
import pandas as pd
import seaborn as sn
from sklearn.metrics import confusion_matrix
import matplotlib.pyplot as plt
import numpy as np
from sklearn.metrics import confusion_matrix
#テストデータに対する混同行列の出力
predict_classes = model_new.predict_classes(x_test)
true_classes = y_test 
def print_cmx(y_true, y_pred):
    labels = sorted(list(set(y_true)))
    cmx_data = confusion_matrix(y_true, y_pred, labels=labels)
    
    df_cmx = pd.DataFrame(cmx_data, index=labels, columns=labels)
 
    plt.figure(figsize = (10,10))
    sn.heatmap(df_cmx, annot=True, fmt='g' ,square = True)
    plt.show()
 
print_cmx(true_classes, predict_classes)
cm = confusion_matrix(true_classes,predict_classes)
#予測結果とラベルが異なる画像の保存
import gc, keras, os
import numpy as np
from keras.models import load_model
from keras.datasets import mnist
from PIL import Image, ImageDraw
pred = model_new.predict_classes(x_test)# Write error images
for i, (p, y) in enumerate(zip(pred, y_test)):
    if p != y:
        image = x_test[i]*255
        img = Image.fromarray(image.reshape((image.shape[0], image.shape[1])).astype(np.uint8))
        print(img)
        img.save(main_path+"result_img/"+"{0}-c{1}-p{2}.png".format(i, y, p)) # c=correct, p=predi
\end{lstlisting}
\subsubsection{出力されたテストデータに対する可視化された混同行列}
モデルをテストデータで評価した結果に対する混同行列を図\ref{confusionmatrix}に示す．
\begin{figure}[H]
    \centering
    \includegraphics[width=\hsize]{img/confusionmatrix.png}
        \caption{テストデータに対する可視化された混同行列}
    \label{confusionmatrix}
    \end{figure}
\subsubsection{考察}
混同行列より腫瘍ありを陽性とし，二値分類問題における評価指標として正解率，適合率，再現率，F値を計算すると以下の図\ref{tab:eval}のようになった．
\begin{table}[htb]
    \begin{center}
        \caption{二値分類問題における評価指標}
        \begin{tabular}{|c|c|} \hline
            正答率&0.885\\  \hline
            適合率&0.911\\ \hline
            再現率&0.871\\ \hline
            F値&0.891\\ \hline
        \end{tabular}
        \label{tab:eval}
    \end{center}
\end{table}


この問題において実際に腫瘍が存在するにも関わらず腫瘍が存在しないと判断する可能性を減らすことが重要となる．そのため，再現率をあげるようなモデルに改良を行っていく必要があると考えられる．
実際に腫瘍ありの画像に対し，腫瘍なしと判断した画像のうち１枚を以下の図\ref{mistake}に示す．また，実際に腫瘍ありの画像に対し正しく腫瘍ありと判断できた画像のうち１枚を図\ref{correct}に示す．
\begin{figure}[H]
    \centering
    \includegraphics[width=10cm]{img/2328-c1-p[0].png}
    \caption{誤診断された腫瘍あり画像}
    \label{mistake}
\end{figure}


\begin{figure}[H]
    \centering
    \includegraphics[width=10cm]{img/1358.png}
    \caption{正しく診断された腫瘍あり画像}
    \label{correct}
\end{figure}
腫瘍とされる部分は画像の左右を見比べて，白く変化している部分であると考えられる．腫瘍が存在するにも関わらず存在しないと判断された図 \ref{mistake}では腫瘍である部分が白く変化している部分を人の目で見ても認識する事は困難である．
これに比べ，正しく診断された図\ref{correct}では左右を見比べて明らかに白く変化している部分が認識できる．
他の画像でも同様の傾向が見られた．よって腫瘍に対し腫瘍でないと誤認識している画像では人の目で見てもあまり認識できないほどの画像であることが考えられる．
\end{document}